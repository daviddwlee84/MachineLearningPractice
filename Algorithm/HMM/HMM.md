# Hidden Markov Model

> Is a kind of a [Directed Graph Model](../CRF/CRF.md#Directed-Graph-Model)

## Overview

The HMM is a generative probabilistic model, in which a sequence of observable X variables is generated by a sequence of internal hidden states Z. The hidden states are not be observed directly. The transitions between hidden states are assumed to have the form of a (first-order) Markov chain. They can be specified by the start probability vector Ï€ and a transition probability matrix A. The emission probability of an observable can be any distribution with parameters Î¸ conditioned on the current hidden state. The HMM is completely determined by Ï€, A and Î¸.

There are three fundamental problems for HMMs:

1. [Given the model parameters and observed data, estimate the optimal sequence of hidden states.](#problem-two-find-the-optimized-state-transition-sequence)
2. [Given the model parameters and observed data, calculate the likelihood of the data.](#problem-one-estimate-observation-sequence-probability)
3. [Given just the observed data, estimate the model parameters.](##problem-three-learning-parameters)

The first and the second problem can be solved by the *dynamic programming* algorithms known as the [**Viterbi algorithm**](#viterbi-algorithm) and the **Forward-Backward algorithm**, respectively.
The last one can be solved by an iterative [**Expectation-Maximization (EM) algorithm**](../EM/EM.md), known as the [**Baum-Welch algorithm**](#baum-welch-algorithm).

> A hidden Markov model is a Markov chain for which the state is only partially observable.

### Quick View

Category|Usage|Methematics|Application Field
--------|-----|-----------|-----------------
Supervised Learning, Unsupervised Learning|Sequential Labeling|Markov Chain|NLP

## Markov Model

*Markov models*|System state is fully observable|System state is partially observable
---------------|--------------------------------|-------------------------------------
**System is autonomous**|Markov chain|Hidden Markov model
**System is controlled**|Markov decision process|Partially observable Markov decision process

### Markov Chain

A Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.

> A Markov chain is a *stochastic process* with the *Markov property*.

The term "Markov chain" refers to the sequence of random variables such a process moves through, with the Markov property defining serial dependence only between adjacent periods (as in a "chain").

#### First-order Markov Chain

#### Time-homogeneous Markov Chain (stationary Markov chain)

#### Example of Markov chain

$P={\begin{bmatrix}0.9&0.075&0.025\\0.15&0.8&0.05\\0.25&0.25&0.5\end{bmatrix}}.$

![wiki example](https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Finance_Markov_chain_example_state_space.svg/400px-Finance_Markov_chain_example_state_space.svg.png)

## The Hidden Markov Model

### Example of Urn and ball

> å£‡å­èˆ‡å°çƒ

![wiki urn](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/HiddenMarkovModel.svg/300px-HiddenMarkovModel.svg.png)

Probabilistic parameters of a hidden Markov model (example)

* X â€” states
* y â€” possible observations => only visible thing in HMM!
* a â€” state transition probabilities
* b â€” output probabilities

### Formula

HMM (ğº) can be defined as (S, V, A, B, ğ¹)

* S: status set
  * $S = \{ 1, 2, 3, \dots, N \}$
* V: output sign set
  * $V = \{v_1, v_2, \dots, v_M \}$
* A: (state) transition matrix (ç‹€æ…‹)è½‰ç§»çŸ©é™£
  * $A_{N\times N} = [a_{ij}]$
  * $a_{ij} = P(q_t=j|q_{t-1}=i)$
* B: output sign probability distribution
  * $B = \{b_j(k)\}$
  * $b_j(k) = P(v_k|j)$
* ğ¹: initial state probability distribution
  * $\pi = \{\pi_i\}$
  * $\pi_i = P(q_1 = i)$

The hidden markov process can be considered as a double stochastic process:

1. the first stochastic process: described with the *state transition matrix* (can't be observed directly)
2. the second stochastic process: defined by the *output probability matrix* => output the observable signs.

### Generate observation sequence

## Problems of HMM

1. For a given HMM ğº = (A, B, ğ¹) and an observation sequence O => Calculate observation probability P(O|ğº)? (i.e. the Probability calculation problem)
2. For a given HMM ğº = (A, B, ğ¹) and an observation sequence O => Find the most possible transition sequence q? (i.e. the Prediction problem)
3. How to get or update the model's parameter by an observation sequence O? (i.e. the Learning problem)
   * based on Maximum Likelihood priniple, how to confirm a set of model parameters that maximize P(O|ğº)

Example of flipping coin: (this will be used to demonstrate the following problems)

ğº = (S, V, A, B, ğ¹)

* S = {1, 2, 3}
* V = {H, T}
* A
    *A*| 1  | 2  | 3
    ---|----|----|----
    1 |0.90|0.05|0.05
    2 |0.45|0.10|0.45
    3 |0.45|0.45|0.1
* B
    *B*| 1  | 2  | 3
    ---|----|----|----
    H |0.50|0.75|0.25
    T |0.50|0.25|0.75
* ğ¹ = {1/3, 1/3, 1/3}

### Problem one: Estimate observation sequence probability

#### Naive (Brute-force) Algorithm

List all the posibility of the state transition sequence then claculate.

1. For the given ğº, calculate P(O)
2. Calculate P(O, q) (joint probability (è¯åˆæ©Ÿç‡) of O and q)
3. Caculate observation sequence probability P(O)

Time complexity:

* multiply: (2T-1)N^T times
* addition: N^T - 1 times

> By the concept of dynamic programming, maybe we can seperate the problem into sub-problem and simplify the problem.
> => Forward Algorithm, Backward Algorithm

#### Forward Algorithm

The forward variable $\alpha_{t}(i)$

$$
\alpha_{t+1}(j) = [\sum_{i=1}^N \alpha_t(i) \alpha_{ij}]b_j(o_{t+1})
$$

1. Initialization (i from 1 to N)
    $$
    \alpha_1(i) = \pi_ib_i(o_1)
    $$
2. Iteration (t from 1 to T-1, j from 1 to N)
    $$
    \alpha_{t+1}(j) = [\sum_{i=1}^N \alpha_t(i) \alpha_{ij}]b_j(o_{t+1})
    $$
3. Termination
    $$
    P(O|\lambda) = \sum_{i=1}^N \alpha_T(i)
    $$

Time complexity:

* multiply: N(N+1)(T-1)+N times
* addition: N(N-1)(T-1) times

Calculate the P(HHT|ğº) of the example:

$a_t(i)$|   H   |   H   |   T
--------|-------|-------|-------
1       |0.16667|0.15000|0.08672
2       |0.25000|0.05312|0.00684
3       |0.08333|0.03229|0.02597

P(HHT|ğº) = 0.11953

#### Backward Algorithm

The backward variable $\beta_{t}(i)$

$$
\beta_t(i) = \sum_{j=1}^N \alpha_{ij}b_j(o_{j+1}\beta_{t+1}(j))
$$

1. Initialization (i from 1 to N)
    $$
    \beta_1(i) = 1
    $$
2. Iteration (t from 1 to T-1, j from 1 to N)
    $$
    \beta_{t}(i) = \sum_{i=1}^N \alpha_{ij}b_j(o_{t+1})\beta_{t+1}(j)
    $$
3. Termination
    $$
    P(O|\lambda) = \sum_{i=1}^N \pi_ib_i(o_1)\beta_1(i)
    $$

> The definition of the backward algorithm is not exact symmetric of the forward algorithm.
> You can define it as a symmetric solution of forward algorithm, but we'll use this algorithm in other problem (the third one)

Calculate the P(HHT|ğº) of the example:

-       |   H   |   H   |   T   | (initial)
--------|-------|-------|-------|-----------
$\beta_t(i)$|$\pi_ib_i(H)\beta_1(i)$|$\beta_1(i)$|$\beta_2(i)$|$\beta_3(i)$
1       |0.04203|0.25219|0.50000|1.00000
2       |0.05074|0.20297|0.58750|1.00000
3       |0.02676|0.32109|0.41250|1.00000

P(HHT|ğº) = 0.11953

### Problem two: Find the optimized state transition sequence

$$
q^* = \arg\max_q P(O, q|\lambda)
$$

> We can also brute-force to solve this problem. Thus we'll use the DP algorithm again.

#### Viterbi Algorithm

> It's a variation of forward algorithm

The Viterbi variable $\delta_t(i)$: At time t and state i. The probability of (observed: $q_1q_2\dots q_{t-1}q_t$ => best state transition sequence: $o_1o_2\dots o_t$)

$$
\delta_t(i) = \max_{q1\dots q_{t-1}} P(q_1q_2\dots q_{t-1}q_t = i, o_1o_2\dots o_t|\lambda)
$$

* base case:
    $$
    \delta_1(i) = \pi_ib_i(o_1)
    $$
* recursive case:
    $$
    \delta_{t+1}(j) = [\max_i\delta_t(i)a_{ij}]b_j(o_{t+1})
    $$
* record the path: $\psi_t(i)$ record the best path from the lastest time (i.e. t-1)

1. Initialization (i from 1 to N)
    $$
    \delta_1(i) = \pi_ib_i(o_1) \\
    \psi_1(i) = 0
    $$
2. Iteration (t from 1 to T-1, j from 1 to N)
    $$
    \delta_t(j) = \max_i[\delta_{t-1}a_{ij}]b_j(o_t) \\
    \psi_t(j) = \arg\max_i \delta_{t-1}(i)a_{ij}
    $$
3. Termination
    $$
    P^* = \max{i}\delta_T(i)
    $$
4. Solve the best path
    $$
    q^*_T = \arg\max_i\delta_T(i)
    $$

Calculate the best state transition sequence of the example:

$\delta_t(i)$|   H   |   H   |   T
-------------|-------|-------|-------
1            |0.16667|0.07500|0.03375
2            |0.25000|0.02812|0.00316
3            |0.08333|0.02812|0.00949

$P^* = 0.03375$

$\psi_t(i)$|$\psi_1(i)$|$\psi_2(i)$|$\psi_3(i)$
-----------|-----------|-----------|-----------
1          |     0     |     1     |     1
2          |     0     |     3     |     3
3          |     0     |     2     |     2

$q^* = 1$

so the best state transition sequence is `(1, 1, 1)`

### Problem three: Learning Parameters

Principle: Maximum Likelihood Estimation - finds the parameter which maximize P(O|ğº)

#### Learning Method

Unknown HMM model ğº, given a observation sequence O

* Supervised Learning:
  * also give State transition sequence (i.e. the answer)
  * pros: easy and good effect
  * cons: need the transition sequence, most of the time need manual tagging => high cost
* Unsupervised Learning
  * no [analytic expression](https://en.wikipedia.org/wiki/Closed-form_expression#Analytic_expression) (è§£æè§£)
  * can only reach local optimal model

#### Naive thought of unsupervised learning

Given initial parameters (A, B, ğ¹)

Because we don't have transition sequence. We're not able to calculate state transition frequency, state output frequency and initial state frequency. => Assume all the state transition sequence are possible!

Then calculate the expectation values of them. And update (A, B, ğ¹) with them.

How to pick weight?

For a state transition sequence q, select P(q|O, ğº)

$$
P(q|O, \lambda) = \frac{P(q, O|\lambda)}{P(O|\lambda)} = \frac{P(q, O|\lambda)}{\sum_qP(q, O|\lambda)}
$$

> But it will only work in theory. We need more efficient algorithm.

In all the possible path, pick $q_t=i$ and $q_{t+1} = j$. The path set:

$$
Q = \{q|q_t=i, q_{t+1} = j\}
$$

The expectation of the transaction (i,j) occur while time t to t+1

$$
\frac{\sum_{q\in Q} P(q, O|\lambda)}{P(O|\lambda)}
$$

#### Baum-Welch Algorithm

> Find the unknown parameters of a hidden Markov model (HMM). It makes use of a forward-backward algorithm.

The variable $\xi_t(i, j)$: For the given model ğº and the observation sequence O. Expetation probability of `time t at state i` => `time t+1 at state j`

$$
\xi_t(i, j) = P(q_t=i, q_{t+1}=j|O, \lambda)
$$

$$
\xi_t(i, j) = \frac{P(q_t=i, q_{t+1}=j|O, \lambda)}{P(O|\lambda)} \\
= \frac{a_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{P(O|\lambda)} \\
= \frac{a_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^N\sum_{j=1}^N \alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}
$$

The variable $\gamma_t(i)$ For the given model ğº and the observation sequence O. Expetation probability of `start from i at time t`

$$
\gamma_t(i) = \sum_{j=1}^N \xi_t(i, j)
$$

Consider all the time

* probability start from state i
    $$
    \sum_{t=1}^{T-1}\gamma_t(i)
    $$
* probability transit from state i to state j
    $$
    \sum_{t=1}^{T-1}\xi_t(i, j)
    $$

Then we can estimate (ğ¹, A, B)

$$
\bar{\pi}_i = \gamma_1(i)
$$

$$
\bar{a}_{ij} = \frac{\sum_{t=1}^{T-1}\xi_t(i, j)}{\sum_{t=1}^{T-1}\gamma_t(i)} = \sum_{t=1}^{T-1}\frac{\xi_t(i, j)}{\gamma_t(i)}
$$

$$
\bar{b}_j(k) = \frac{\sum_{t=1}^{T}\gamma_t(j) \times \delta(o_t, v_k)}{\sum_{t=1}^{T}\gamma_t(j)}
$$

($\delta(o_t, v_k)$ = 1 when $o_t = v_k$, otherwise = 0)

1. Initialize (ğ¹, A, B) to all 1.
2. Iterative calculate new parameter ($\bar{\pi}, \bar{A}, \bar{B}$)
3. Update them and calculate again...
4. Until converge

> Baum-Welch Algorithm is a kind of EM algorithm
>
> * E-step:
>   * Calculate $\xi_t(i, j)$ and $\gamma_t(i)$
> * M-step:
>   * Estimate model $\bar{\lambda}$
> * Termination condition
>   $$
>   |\log P(O|\bar{\lambda}) - \log P(O|\lambda)| < \epsilon
>   $$

### Implementation of HMM

Floating point overflow problem

* for Forward algorithm
  * scaling factors (æ”¾å¤§å› å­) & log
* for Viterbi algorithm
  * log
* for Baumâ€“Welch algorithm
  * scaling factors

## Resources

### Wikipedia

* [Stochastic process](https://en.wikipedia.org/wiki/Stochastic_process)
* [Markov chain](https://en.wikipedia.org/wiki/Markov_chain)
  * [Examples of Markov chains](https://en.wikipedia.org/wiki/Examples_of_Markov_chains)
* [Markov model](https://en.wikipedia.org/wiki/Markov_model)
  * [Hidden Markov model](https://en.wikipedia.org/wiki/Hidden_Markov_model)
    * [Forward algorithm](https://en.wikipedia.org/wiki/Forward_algorithm)
    * [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm)
    * [Baumâ€“Welch algorithm](https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm) ([Forwardâ€“backward algorithm](https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm), [Expectationâ€“maximization algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm))

### Article

* [éš±é¦¬çˆ¾ç§‘å¤«æ¨¡å‹ (HMM) ç°¡ä»‹â€”â€”è‡ªç„¶èªè¨€è™•ç†è©æ€§æ¨™æ³¨](https://nlpcs.com/article/hmm)

### Paper

* [Numerically Stable Hidden Markov Model Implementation](http://bozeman.genome.washington.edu/compbio/mbt599_2006/hmm_scaling_revised.pdf)

### Package

#### HMMLearn

* [Github hmmlearn/hmmlearn](https://github.com/hmmlearn/hmmlearn)
* [Document](https://hmmlearn.readthedocs.io/en/stable/)

#### HanLP

* [Python API Github](https://github.com/hankcs/pyhanlp)
* [Official Main Page](http://hanlp.com/)

#### GHMM

* [GHMM Library](http://www.ghmm.org/)
